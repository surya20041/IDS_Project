{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input folder and output CSV file paths\n",
    "# Change to your input folder path\n",
    "input_folder = r\"C:\\Users\\Sagar\\Python files_Jupiter\\Git Repo Local\\IDS_IPS Softmax Classification\\Datasets\\MachineLearningCVE\"  \n",
    "output_csv_path_train = r\"C:\\Users\\Sagar\\Python files_Jupiter\\Git Repo Local\\IDS_IPS Softmax Classification\\Datasets\\Final Datasets\\train.csv\"\n",
    "output_csv_path_test = r\"C:\\Users\\Sagar\\Python files_Jupiter\\Git Repo Local\\IDS_IPS Softmax Classification\\Datasets\\Final Datasets\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {\n",
    "    \"BENIGN\": \"BENIGN\",\n",
    "    \"DDoS\": \"DDoS\",\n",
    "    \"PortScan\": \"PortScan\",\n",
    "    \"Bot\": \"Bot\",\n",
    "    \"Infiltration\": \"Infiltration\",\n",
    "    \"Web Attack � Brute Force\": \"Web Attack - Brute Force\",\n",
    "    \"Web Attack � XSS\": \"Web Attack - XSS\",\n",
    "    \"Web Attack � Sql Injection\": \"Web Attack - Sql Injection\",\n",
    "    \"FTP-Patator\": \"FTP-Patator\",\n",
    "    \"SSH-Patator\": \"SSH-Patator\",\n",
    "    \"DoS slowloris\": \"DoS slowloris\",\n",
    "    \"DoS Slowhttptest\": \"DoS Slowhttptest\",\n",
    "    \"DoS Hulk\": \"DoS Hulk\",\n",
    "    \"DoS GoldenEye\": \"DoS GoldenEye\",\n",
    "    \"Heartbleed\": \"Heartbleed\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary where keys are unique values, and values are the desired numbers of records\n",
    "num_records_dict = {\n",
    "    \"BENIGN\": 50000,\n",
    "    \"DDoS\": 50000,\n",
    "    \"PortScan\": 50000,\n",
    "    \"FTP-Patator\": 50000,\n",
    "    \"SSH-Patator\": 50000,\n",
    "    \"DoS slowloris\": 50000,\n",
    "    \"DoS Slowhttptest\": 50000,\n",
    "    \"DoS Hulk\": 50000,\n",
    "    \"DoS GoldenEye\": 50000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:{'BENIGN': 320000, 'DDoS': 40000, 'PortScan': 40000, 'Bot': 0, 'Infiltration': 0, 'Web Attack - Brute Force': 0, 'Web Attack - XSS': 0, 'Web Attack - Sql Injection': 0, 'FTP-Patator': 6350, 'SSH-Patator': 4717, 'DoS slowloris': 4636, 'DoS Slowhttptest': 4399, 'DoS Hulk': 40000, 'DoS GoldenEye': 8234, 'Heartbleed': 0} test:{'BENIGN': 80000, 'DDoS': 10000, 'PortScan': 10000, 'Bot': 0, 'Infiltration': 0, 'Web Attack - Brute Force': 0, 'Web Attack - XSS': 0, 'Web Attack - Sql Injection': 0, 'FTP-Patator': 1588, 'SSH-Patator': 1180, 'DoS slowloris': 1160, 'DoS Slowhttptest': 1100, 'DoS Hulk': 10000, 'DoS GoldenEye': 2059, 'Heartbleed': 0}\r"
     ]
    }
   ],
   "source": [
    "train_dict={\n",
    "    \"BENIGN\": 0,\n",
    "    \"DDoS\": 0,\n",
    "    \"PortScan\": 0,\n",
    "    \"Bot\": 0,\n",
    "    \"Infiltration\": 0,\n",
    "    \"Web Attack - Brute Force\": 0,\n",
    "    \"Web Attack - XSS\": 0,\n",
    "    \"Web Attack - Sql Injection\": 0,\n",
    "    \"FTP-Patator\": 0,\n",
    "    \"SSH-Patator\": 0,\n",
    "    \"DoS slowloris\": 0,\n",
    "    \"DoS Slowhttptest\": 0,\n",
    "    \"DoS Hulk\": 0,\n",
    "    \"DoS GoldenEye\": 0,\n",
    "    \"Heartbleed\": 0,\n",
    "}\n",
    "test_dict={\n",
    "    \"BENIGN\": 0,\n",
    "    \"DDoS\": 0,\n",
    "    \"PortScan\": 0,\n",
    "    \"Bot\": 0,\n",
    "    \"Infiltration\": 0,\n",
    "    \"Web Attack - Brute Force\": 0,\n",
    "    \"Web Attack - XSS\": 0,\n",
    "    \"Web Attack - Sql Injection\": 0,\n",
    "    \"FTP-Patator\": 0,\n",
    "    \"SSH-Patator\": 0,\n",
    "    \"DoS slowloris\": 0,\n",
    "    \"DoS Slowhttptest\": 0,\n",
    "    \"DoS Hulk\": 0,\n",
    "    \"DoS GoldenEye\": 0,\n",
    "    \"Heartbleed\": 0,\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame to store the extracted rows\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each CSV file in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        input_csv_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Read the current CSV file into a DataFrame\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        \n",
    "        # Get the last column name (assuming the last column is the one you want)\n",
    "        last_column_name = df.columns[-1]\n",
    "\n",
    "        df[last_column_name] = df[last_column_name].map(filter_dict)\n",
    "\n",
    "        # Iterate through unique values in the last column\n",
    "        for value, num_record in num_records_dict.items():\n",
    "            # Filter rows for the current value in the last column\n",
    "            filtered_df = df[df[last_column_name] == value]\n",
    "\n",
    "            # If the number of records available for this value is less than the desired number, use all available\n",
    "            num_records = min(num_record, len(filtered_df))\n",
    "\n",
    "            train_dict[value]+=int(num_records*0.8)\n",
    "            test_dict[value]+=num_records-int(num_records*0.8)\n",
    "\n",
    "            # Randomly select the specified number of rows to extract\n",
    "            train_rows = filtered_df.iloc[:int(num_records*0.8),:]  # Use a fixed random_state for reproducibility\n",
    "            test_rows = filtered_df.iloc[int(num_records*0.8):num_records,:]\n",
    "            # Append the sampled rows to the extracted DataFrame\n",
    "            train_df = pd.concat([train_df, train_rows])\n",
    "            test_df = pd.concat([test_df, test_rows])\n",
    "\n",
    "            print(f\"train:{train_dict} test:{test_dict}\",end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the output CSV file already exists\n",
    "if os.path.isfile(output_csv_path_train):\n",
    "    # Read the existing data\n",
    "    existing_df = pd.read_csv(output_csv_path_train)\n",
    "    # Append the new data to the existing data\n",
    "    train_df = pd.concat([existing_df, train_df], ignore_index=True)\n",
    "\n",
    "# Write the extracted DataFrame to the output CSV\n",
    "train_df.to_csv(output_csv_path_train, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the output CSV file already exists\n",
    "if os.path.isfile(output_csv_path_test):\n",
    "    # Read the existing data\n",
    "    existing_df = pd.read_csv(output_csv_path_test)\n",
    "    # Append the new data to the existing data\n",
    "    test_df = pd.concat([existing_df, test_df], ignore_index=True)\n",
    "\n",
    "# Write the extracted DataFrame to the output CSV\n",
    "test_df.to_csv(output_csv_path_test, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
